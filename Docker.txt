############# WHAT IS A CONTAINER ############

To understand what a container is 1st we need to know how our os works, Here most os have something called "Kernel" which is a running software process that governs or control access between all the programs running on our computer (Software) and all the physical hardware that is connected to our computer (Hardware). 

So we can have software like "chrome, terminal, spotify, nodeJS etc" and hardware like "cpu, memory, hard disk etc". If we are writing a nodejs code which is getting save in a hard drive then it is not nodeJS that is speaking directly to hard drive. Instead nodeJS says to our kernel "Hey, i want to write a file to the hard drive". The kernel then takes that information and eventually persists it to the hard drive. 

This running programs (Software) interacts with system kernel through something called "SYSTEM CALL" to indirectly interact with Hardware.

Now lets say we have 2 programs running on our computer "Chrome and nodeJS". Lets say for chrome to work properly need (Python v2) and nodeJS (Python v3) but in our hard disk we have only Python v2 installed, So chrome will work properly and nodeJS will not. To solve this issue we use os feature called "Namespacing". 

With namespacing, we can look at all of the different hardware resources connected to our computer and we can segment out portions of those resources. So we can create a segment of our hard disk specifically for Python v2 and Python v3. So anytime chrome and nodeJS issue a system call to kernel to read information off the hard drive, The kernel will look into the system call and try to figure out which process is it coming from, So if kernel see request coming from Chrome then it will direct it to the segment of hard drive contains (Python V2).

Well namespacing is not only limited to hardware, It can be also used for network (To restrict the network device that are available), To restrict the ability to talk to other processes.

@@@@@@ Namespacing : A technique used to isolate or segment system resources for different processes, ensuring they operate independently without interfering with each other.@@@@@@

Similar to namespacing we have A "cgroup" (Control Group) is a Linux feature that allows resource management by controlling how much CPU, memory, disk I/O, and network bandwidth a process (or group of processes) can use.

So namespacing is for saying, "Hey this area of hard drive is for this process", A cgroups can be used to limit the amount of memory, cpu and resources that process can use. 

So the entire section of running process (Chrome) and little segment of a resource that it can talk to (Python v2) is what we refer to as a container.

@@@@@@@ Container : It is a process or a set of processes that have a grouping of resources specifically assigned to them. A Docker container is a lightweight, portable, and isolated environment that runs application along with all their dependencies, libraries, and configurations. @@@@@@@@@


@@@@@@@@@@@@@@ Now how this image leads to creation of container :: @@@@@@@@@@@@@@@@

An image is a FS (file system) snapshot. It is like a copy paste of a very specific set of directories or files. So we can have a image that contains just chrome and python, An image also contains a specific startup command. 

When we takes an image and turn it into a container, The kernel will isolate a little section of the hard-drive and make it available to just that container. So after the segment in hard-drive is created the image FS snapshot is taken and placed into that little segment. Then the startup command which is in our case is "> Run Chrome" will be executed and process is establised that runs in a isolated environment with its own set of resources. 

A Docker image is a lightweight, standalone, and executable package that contains everything needed to run an application, including:
✅ The application code
✅ System libraries and dependencies
✅ Environment settings and configurations
✅ A base operating system (like Alpine, Ubuntu)

Docker images are immutable, meaning once created, they don't change. Containers are running instances of these images.

1️⃣ Docker Image – A blueprint that defines what goes into the container (e.g., OS, app code, libraries).
2️⃣ Docker Container – A running instance of a Docker image.
3️⃣ Docker Engine – The runtime that manages containers.


############## HOW'S DOCKER RUNNING ON OUR COMPUTER ###############

Here these "namespacing" and "cgroups" are features of linux os, It is not available by-default to other os like windows, macOS etc. 

So how we are running docker in our window OS, When we install docker desktop for windows we have also installed linux virtual machine, So as long as docker desktop is running the linux virtual machine is also running. Inside this virtual machine is where all these containers are going to be created. Inside this virtual machine we have linux kernel that is going to be hosting running processes inside of containers and that linux kernel that is going to be in charge of limiting and controlling access to different hardware resources. 

We can see this linux virtual machine in practice by going to WSL terminal and write "docker version" and under "Server:  Engine:" section we have an OS entry of linux there. 


########### DOCKER RUN IN DETAIL ############

The docker run command is used to create and start a container from a Docker image. Here the "docker" (Reference to docker client) "run" (Try to create and run a container) "<image name>" (Name of image to use for this container). 

Here when we run "docker run hello-world" command behind the scene, Chances are somewhere on our hard disk is an image that has that file system snapshot with one single program inside of it which is called "hello-world". When we run this command we took that snapshot from hard drive and stuck it into that container with other group of resources and then execute the command which start the running process in container called "hello-world".

Here anytime we run "docker run <image-name>" command we not only get the FS snapshot but we also get the default command that is supposed to be executed after the container is created. 

So docker run has a variation in which we can override this default startup command. SO ::
"docker" "run" "<image-name>" "command" (overriden command to be executed in place of default one inside the image when container is started). EX ::

" docker run busybox echo hello there ", Here the "echo hello there" is the overidden command that will execute inside the container and print hi there in the terminal. Now little variation on this command is " docker run busybox ls ", Here the " ls " will print out all the files and folders inside of a given directory. So we will get " bin, dev, etc, home etc " in the terminal and these directories are not part of our system but exists inside that container FS only. 

Here again when we try to create container out of an image (busybox) it will have some default FS snapshot with default command. The busybox image has a default folders of " bin, dev, etc, home etc ". So when we create container out of busybox image we take its FS snapshot and stick ti as a folder inside the container and then the command "ls" list out all the files and folders inside of hard-drive which then prints " bin, dev, etc, home etc.". 

If we try to run " docker run hello-world echo hi there " and " docker run hello-world ls " we will get errror because "ls" and "echo" are 2 programs that does not exist inside of "hello-world" image FS snapshot but in case of "busybox" they do exist. In case of "hello-world" image only a single program exist which runs and print out the message in terminal but no "echo" and "ls" executable are present in it. 


############ LISTING RUNNING CONTAINERS ###############

To list all the container currently running on our machine command is " docker ps ". Right now if we execute this command we will see list of headers in a table but no entry is present because at this point we have only been running images or creating containers that run very quickly and immediately close down. 

To make ccontainer run a bit longer use command " docker run busybox ping google.co.in ", The container tries to reach google.co.in and displays network response times which confirms that container has internet access.  

Now we can do " docker ps " command and see the table with an entry, We have a "CONTAINER ID" ( every container as a unique id by which we can perform operation on specific container ), "IMAGE" ( Image that is used for the container ), "COMMAND" ( command running inside the container ), "CREATED" ( How long ago the container was created ), "STATUS" ( Whether the container is running or stopped ), "PORTS" ( List of ports that have been opened for outside access ) and "NAMES" ( randomly generated name assigned to the container. )

To lists all the containers, including both running and stopped ones that have ever been created command is " docker ps --all " .

@@@@@@@ NOTE :: The common use case of " docker ps " command is not only to see which containers are running but to get the ID of the running container ( CONTAINER ID ) because we need to issue commands on a specific container for which we need its ID.


######## CONTAINER LIFECYCLE ##########

Creating a container and actually starting it up are 2 different processes. EX ::

docker run = docker create + docker start , Here docker run command is identical to running other 2 command which is docker create and docker start. 

the " docker create <image-name> " is used to create a container out of an image without starting it up and " docker start <container-id> " is used to start the container using its ID. 

Here initially we have a image in which there is a FS snapshot present with a startup command, When creating a container is where we take that FS snapshot and put inside segment of hard-drive resource available for that container, To start the container is where we actually take that startup command and execute it which will start the process like messages in case of hello-world image.

Here the " docker create hello-world " returns the "CONTAINER ID" of the newly created container which we can use to start the container using " docker start CONTAINER ID ". 

Here we have another command " docker start -a CONTAINER ID " similar to " docker start CONTAINER ID ", The difference is with " -a " flag is what makes docker actually watch for output from the container and print it out to our terminal. Without " -a " flag it starts the container in the background (detached mode) and does NOT show any output from the container.


########### RESTARTING, REMOVING AND STOPPED THE CONTAINERS ##############

Here once the container is executed and stopped does not mean it cannot be used again, We can start the stopped container again using " docker start -a CONTAINER-ID " to start a stopped container using the ID and logs the output on terminal.

Here lets say we already have a container created for busybox image so FS snapshot and startup command " echo hello there " are there inside the container. The container after executed the command exited it naturally, We then ran the docker start command which will issue the running process or primary startup command for the 2nd time inside the container 

It means if we have a container already been created, We cannot replace the default command used to start that container for the 1st time. So doing this " docker start -a CONTAINER-ID COMMAND " will give us error.

Similar to starting the stopped container we can restart the stopped or already running container using " docker restart CONTAINER-ID ", It stops the container (if running) and then starts it again, It always forces a restart, even if the container is already running. 

Here the stopped container takes memory of the system disk so it is sometime better to remove the stopped container from the memory if not of use in future. The command is " docker rm CONTAINER-ID ".

To delete all the containers use command " docker system prune ", This will give us warning as this command not only remove the stopped containers but also other things like "build cache" ( Any image pulled from docker hub ). To clean Up unused Docker Resources.

Here so far in order to print the output of started container in the terminal we use " -a " flag with " docker start -a CONTAINER-ID ", Sometime we come across scenario in which we forgot to add that " -a " flag while starting the container and starting took 5-10 minutes so doing the start again with " -a " flag just to view the logs is frustrating so to get the output logs without adding this " -a " flag we use " docker logs CONTAINER-ID " command which returns the output logs of the container based on ID.

Here running docker logs does not mean re-running and restarting the container. It is just a way to get all the logs that have been emitted from the container.

Here previously we see the ping command to google using " docker run busybox ping google.co.in " which continously makes network request to google for some time. Now in order to stop this container from running further we can use " docker stop <CONTAINER-ID> " or " docker kill <CONTAINER-ID> " Both commands terminate a running container, but they do it differently.

docker stop <CONTAINER-ID> ::

✅ When issue this command a hardware signal called "SIGTERM" ( signal terminate ) is sent to primary process of the running container which when get it will shut donw its process gracefully.
✅ A lot of programming languages like "nodeJS" allow us to listen to signal like "SIGTERM", "SIGINT" ( crtl + c ) to perform cleanup task. EX ::

process.on("SIGTERM", () => {
  console.log("Received SIGTERM, cleaning up...");
  process.exit(0);
});

process.on("SIGINT", () => {
  console.log("Received SIGINT, shutting down gracefully...");
  process.exit(0);
});

✅ Gives the container time to clean up before shutting down.
✅ If the container doesn’t stop within 10 seconds, it forcefully kills it by issuing the "SIGKILL" signal.


docker kill <CONTAINER-ID> ::

✅ Immediately stops the container by sending a "SIGKILL" ( signal kill ) signal.
✅ Does not allow cleanup (any ongoing processes are terminated instantly).
✅ Useful when a container is unresponsive or needs to be shut down ASAP.

So if i run " docker run busybox ping google.co.in " it will create and start the container which continue to run long and then write command " docker stop <container-id> " of busybox then it will wait for 10 sec before killing using docker kill command. This is because ping command does not know how to handle "SIGTERM" signal. 


########### MULTI-COMMAND CONTAINERS #############

Lets install redis-server and redis-cli locally without docker and on one terminal we will start "redis-server" and on 2nd we will start "redis-cli" which is used to poke around and interact with "redis-server".

Now lets start redis-server using docker by " docker run redis ", Now we wil start redis-cli to interact with this created redis-server but if we try to open redis-cli on 2nd terminal we will get error " Could not connect to redis server ". This is because we have 1st started a new container in which "redis-server" is currently running and that redis-server is running only inside that container so from outside if i try to use redis-cli to interact with redis-server i just cannot. So if we want to start the redis-cli to communicate with redis-server inside the container we have to start a 2nd program or 2nd process inside the container.

To start multiple process inside the container we can use " docker exec -it <CONTAINER-ID> <COMMAND> ". The docker exec command allows you to run commands inside an already running container.

Now 1st run docker run redis so a container with main process of redis-server is up and running, Then to start 2nd program which is redis-cli inside that same container we use " docker exec -it <CONTAINER-ID> redis-cli ", By this we have started a 2nd program inside that container and " -it " flag allow us to provide input from keyboard to inside that running container for processing. Without the "-it" flag the redis-cli will be started but we will not get ability to communicate with the container. 

@@@@@@@ 

NOTE :: The purpose if " -it " flag is very important in the world of docker.

In linux environment as these containers even if we are in windows still running inside linux VM, When we create a container and inside it has a process, Every process in linux environment has 3 communication channels attached to it namely ( STDIN, STDOUT, STDERR ). These channels are used to communicate information either into the process ( STDIN ) or out of the process ( STDOUT ). 

Here this "-it" flag is combination of 2 other flag which is "-i" ( When execute new command inside the container, We want to attach our terminal to STDIN channel of the running process ) and "-t" ( Allow us to view the input we are passing and coming out of the channel in a nice format or pseudo-terminal for a better interactive session ) , But in general we can use it together by " -it " flag.

@@@@@@@@

Sometime we need to get a shell or terminal access of our running container. In other words, we can run commands inside the container without having to re-run "docker-exec" again and again. 

To get the shell or terminal in the context of our container " docker exec -it 74947b906c99 sh ", By this we will get a another terminal ( # ) in which we can write any commands expected to run in a typical " UNIX/LINUX " environment. We can now directly run program like " redis-cli " without having to re-run docker exec. And when we are done just hit " CRTL + D " or "exit" instead of  " CRTL + C " to exit. 

Here this "sh" is the name of the program that is being executed inside the container. Sh is a command processor or a shell that allows you to type command in and execute them inside the container. Similar to "sh" (UNIX) we have "git bash, powershell" (WINDOWS), "bash" (MACOS). 

Most of the container we are going to work with have "sh" program already been included, Some more complete version of containers also have "bash" program included. 

Here we can also use docker run command with "-it" flag to start up a shell immediately when a container first starts up, Offcourse this will displace or keep any other default start-up command from running. But sometime it is usefull to get a empty container with a shell inside of it to just poke around and not have any other processes running. 

So " docker run -it busybox sh ", This will runs an interactive shell (sh) inside a BusyBox container in which we can write linux commands.


######### CONTAINER ISOLATION ##########

We know each container runs independently from others and the host system, preventing interference. Docker achieves this using namespaces and cgroups. They don't automatically share filesystem. EX ::

Start 2 different busybox container shell " docker run -it busybox sh ", Now on 1st container sh lets create a file " touch hithere " which we can see using " ls ". Now on 2nd terminal if we do ls we will not see any "hithere" file which confirms that these 2 containers have different file system and there is no sharing of data between them unless we form up a connection between them. 


######### DOCKER IMAGES ##########

So far we have been using images created by other developers like "hello-world", "redis" and "busybox", Download them onto our local machine from docker HUB and then created container out of them. But we can create our own custom image to run our own application inside of our own personalized container.

TO CREATE OUR OWN IMAGE ::

"DOCKERFILE" ( A text file that is going to have a couple of lines of configurations inside of it which defines how our container behaves ) ->  "DOCKER CLIENT" ( Once dockerFile is created it will be passed to docker client which then transfer it to docker server )  ->  "DOCKER SERVER" ( This is what doing the heavy lifting for us, It takes the dockerFile goes through the configurations inside of it and then build a usable image that we can use to start up a container )

CREATING A DOCKERFILE ::

Specify a base image  ->  Run some commands to install additonal programs, packages and dependencies that we need to create and start-up our container  ->  Specify the start-up command which will executed to boot up or start the container.


########## BUILDING A DOCKERFILE ############

We will make a dockerFile that is going to create an image that is going to run "redis-server". 

Now we will create our image in "redis-image" directory. There just create a single "Dockerfile" file. Now after writing configurations in it just write command "docker build ." and this will create the image we need to create the container, Then use the image-id and run "docker run <IMAGE-ID>" to create container out of that image.

DOCKERFILE-TEARDOWN ::

Here every 1st word in dockerFile is refer as an instruction, This instruction is telling the docker server to do some very specific preparation on the step of creating an image. 

The instruction "FROM" is used to specify the docker image that we want to use as a base image. 
The instruction "RUN" is used to execute some commands while our image is getting created or in build process.
The instruction "CMD" is used to defines the default start-up command that runs when a container is started from this image

@@@@@ NOTE :: The "CMD" instruction is executed only when the container runs, unlike "RUN", which is executed during image build. @@@@@

Every line of configuration that we add in dockerFile is always start with "instruction". These " FROM, RUN AND CMD " are the most important instructions out of many out there. 

With each instructions we have provided some argument ::

1. FROM alpine => It means alpine Linux is used as the base image.

2. RUN apk add --update redis => Using apk, which is Alpine's package manager we are installing redis using "apk add redis" and the "--update" flag ensures the latest package versions are installed  

3. CMD ["redis-server"] => Runs Redis server as a main process when the container starts.  


######### WHAT IS A BASE IMAGE ###########

WRITING A DOCKERFILE === GIVEN A COMPUTER WITH NO OS AND BEING TOLD TO INSTALL CHROME

In order to understand what is a base image lets say we need to install google chrome on a computer with no OS. So to acheive this 1st when we open our computer we will see an error that say no os present so 1st -> we will need to install os as without it there will be no default browser to download chrome from, no file-folder explorer to put the download and no way to run that installer. So as we see my further steps depend on installing the os 1st. 

So these steps are similar to what we have done in dockerFile, When we specify base image of "Alpine" is similar to installing os by default. By-default when we create an image we have an empty image with nothing inside there, no infrastructure, no programs to navigate around in files or folders. So purpose of having a base image is to give us an initial starting point or an initial set of programs that we can use to further customize our image. 

So meaning of "FROM alpine" is we wanted to use alpine image as a initial os or starting point for the image we are creating. Here the reason of using alpine as a base image is similar to using widows, macos, linux as a os because they provide some programs that we want to use. So using alpine because it provides default programs like "apk" etc that allow us to install redis easily. 

Here the command "apk add --update redis" has nothing to do with docker infact it is not a docker command but "APK" ( Apache package manager ) that comes pre-installed with alpine image that we can use to install dependencies or packages which is in our case is "redis". 


######## IMAGE BUILD PROCESS IN DETAIL ##########

When we run "docker build ." it means giving our dockerFile to docker CLI. The build command is what we use to take a dockerFile and generate an image out of it. The "." in docker build is what we called build context. The build context is essentially the set of files or folders that belong to our project and that we want to encapsulate or wrap in the container.

After running the command we see a bunch of output in the terminal, For every line of configuration present in dockerFile a step will be shown in the build process. 

Here in the build process the 1st step is FROM alpine means the docker server looked at our local build cache and it checked to see if has ever downloaded an image called "alpine". But after step one whatever further step take place will be done inside some sort of temporary container.

Here behind the scene for step 2 or 3 what happens, Since after step 1 we already have alpine image FS snapshot and start-up command available. When docker server see the 2nd step which is "RUN apk add --update redis" it looked back at the last step just occured and looked at the image come out of the last step which is alpine image, Then using that image alpine it creates a brand new temporary container out of it, So we can imagine we took the entire FS snapshot of alpine image put it in segment of hard-drive available for that container and then the command "RUN apk add --update redis" will be executed as a primary process in that container. Now during the installation of redis we just say a new folder "redis" is created in the container directory, So after command is processed the container is stopped and took the FS snapshot of that container and save it in temporary image with some id, So output of step 2 is just new temporary image with updated FS snapshot. Now at step 3 again we will be looking at the temporary image created during previous step 2, So again temporary container with putting of FS snapshot in hard-drive will take place but now the command is "CMD" which this temporary container does not execute right away unless if there is any other command, But just set up the command as main process for real start-up. At last this temporary container will also be stopped and output which is FS snapshot and start-up command of it will be saved in final image.

FOR MORE UNDERSTANDING WATCH "https://www.udemy.com/course/microservices-with-node-js-and-react/learn/lecture/19223722#overview"


########## IMAGE REBUILD WITH CACHE ###########

Now lets add another command which is " RUN apk add --update gcc " and run the docker build command. Now we will notice on step 1 we are not going to docker HUB as alpine is already available in local image cache but in step 2 we are not installing the "redis" using apk again but just a message "USING CACHE", It means docker knows that from step 1 to step 2 nothing has changed since the last time we ran docker build. So docker already has alpine image and temporary image of step no 2 of previous build so it will not again install it, But in step 3 which is new configuration added docker will do install "gcc" and cannot use build cache anymore. But good thing is if we re-run docker build this time the step 3 will also be used from cache.  

So anytime we make a change to the dockerFile we have to only rerun the series of step from the change line to everything down. EX :: just put the "gcc" install above the "redis" and do build again. Now even if both gcc and redis is installed at the end the series of step has been changed as now we are installing gcc before redis so from step 2 no cache will be used.


####### TAGGING AN IMAGE ########

Here right now we are creating container from custom image using "docker run <image-id>" and having to know image id for creating is kind of a pain so for this we tag an image in docker.

Tagging is used to identify and version an image. It helps in managing multiple versions of an image, especially when pushing to a container registry. The command is ::

docker build -t "tag-name" . , Here the convention for tagging the image is " -t <DOCKER-ID> / <PROJECT-NAME> : VERSION ", Here the docker-id is docker HUB username which we can find using command " docker login ". 

So lets tag the image using " docker build -t bishtaman/redis:latest . ". After this we will be shown the tag name at the terminal. Now anytime we try to create container out of that image we can create it without using image-id but using the tag-name, " docker run bishtaman/redis " and we do not need to add ":latest" at the end as by default it will use the latest version of the image always if not mentioned. 

Here for some image like "hello-world, redis and busybox" have simiplied name instead of any tag and version this is because those are community images which is created by people and kind of open source. Anytime we are creating an image of our own we are always going to tag it with DOCKER-ID.

Here in the entire process of tagging the last thing which is " :version " at the end is the actual tag. 


######## MANUAL IMAGE GENERATION USING DOCKER COMMIT ##########

Here imm the flow of build of image we saw that an image is used to create a temporary container and that container is used to create next image for another temporary container. So we can say that image is used to create a container and we can also say from container we can create the image as well. We can create a container manually run some command on it that changes it file system and then generate a usable image using that FS snapshot of that container using "DOCKER COMMIT". The docker commit command is used to create a new image from a modified container. This allows you to save changes made inside a running container as a reusable image. EX ::

1st create a container using " docker run -it alpine sh ", Now inside the shell manually install redis using " apk add --update redis " which will modify the container file system. Now on 2nd terminal we will use docker commit command to take the FS snapshot of the running container and assign a default start-up command to it and then create a new image out of it.

" docker commit -c "CMD 'redis-server'" <CONTAINER-ID> ", This will return the image id of a new image created from that running container. Here the image-id is always big so one shortcut to avoid copying whole id is to copy only starting portion or substring of it as docker check the ID based on susbtring pattern. 

@@@@@@ 

NOTE :: In general we use dockerFile to create image instead of docker commit because it allow us to run series of steps easily 

Here we cannot use this command in window " docker commit -c 'CMD ["redis-server"]' <CONTAINER-ID> ", we will get error so have to use this version " docker commit -c "CMD 'redis-server'" <CONTAINER-ID> "

@@@@@@@


####### NODE-JS APP ON CONTAINER ########

We will create a nodeJS application wrap it inside a docker container and then be able to access that web application from the browser running on our system. We will see how to put and run nodeJS app inside the docker container. EX ::

Create nodeJS app  ->  Create a dockerFile ( To create an image that is going to be used to run our web server inside the container )  ->  Run image as a container on our local machine  ->  Connect to web app inside the container from our browser.

In this web app we are intentionally creating some error ::

1. Wrong base image => Right now our base image is " FROM alpine " and after that we are installing package using " npm install " that requires npm means " nodeJS " to be installed in our container OS. But by default in alpine linux there is no such version of nodeJS come pre-installed so we have to find different base image that have pre-installed node which we can find from docker HUB which is " node:18-alpine ", It means we get alpine os with nodeJS installed on it that allow us to "npm" command.

Now if we run " docker build . " command we still gets error " /package.json " not found, As this file is required by npm install command to install dependency looks like container does not have this file. 

2. Missing file => Here right now in our container creating from the image we have only FS snapshot of alpine node 14 image and then we are running npm install command without package.json in the snapshot. By-default when building the image none of the file present in our project or dockerFile directory is available inside the container. So to add them inside the container we will add one more line of configuration to make sure our source-code is available inside the container " COPY ./from ./to ", Where ./from ( path of folder or file on our local machine relative to build context ) and ./to ( path of folder to copy our stuff to inside the container ). SO ::

" COPY ./ ./ " 

But we also need to set working directory using " WORKDIR /usr/app  " command, It is used to set the working directory inside the container. Any subsequent commands (such as COPY, RUN, or CMD) will be executed relative to this directory.

@@@@@ 

NOTE :: Most of the image use alpine as base image because of its small size, security, and efficiency.

@@@@

Now instead of generating temporary image again and again we will just tag the image " docker build -t bishtaman/simpleweb " and now run the container using " docker run bishtaman/simpleweb " and try to open app at "localhost:8080" we 
error and could not reach port 8080.

What is happening right now our browser makes request to localhost:8080 in our local machine and by-default no trafic coming to our machine network is routed into the container, So the container has its own isolated set of ports that can receive traffic. To make sure any trafic from any computer redirected into the container we have to do "PORT MAPPING". 

Port mapping in Docker allows you to expose a container's internal ports to the host machine so that external clients can access the services running inside the container.






